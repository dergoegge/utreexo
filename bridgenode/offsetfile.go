package bridgenode

import (
	"bufio"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"os/exec"
	"path/filepath"
	"sync"

	"github.com/mit-dci/utreexo/util"
)

type offsetFileResumeData struct {
	// the last height that was indexed
	lastOffsetHeight int32
	// the number of blocks in `lastFileNum` that are already indexed
	blocksIndexedInLastFile uint32
	// the file number to resume on
	fileNum uint32
	// Map to store Block Header Hashes for sorting purposes
	// blk*.dat files aren't in block order so this is needed
	nextMap map[[32]byte]RawHeaderData
}

func (resumeData *offsetFileResumeData) ReadFrom(r io.Reader) (n int64, err error) {
	n = 8
	resumeData.lastOffsetHeight = 0
	resumeData.blocksIndexedInLastFile = 0
	resumeData.nextMap = make(map[[32]byte]RawHeaderData)
	err = binary.Read(r, binary.BigEndian, &resumeData.lastOffsetHeight)
	if err != nil {
		if err == io.EOF {
			err = nil
		}
		return
	}

	err = binary.Read(r, binary.BigEndian, &resumeData.blocksIndexedInLastFile)
	if err != nil {
		if err == io.EOF {
			err = nil
		}
		return
	}

	err = binary.Read(r, binary.BigEndian, &resumeData.fileNum)
	if err != nil {
		if err == io.EOF {
			err = nil
		}
		return
	}

	var mapLen uint32
	err = binary.Read(r, binary.BigEndian, &mapLen)
	if err != nil {
		if err == io.EOF {
			err = nil
		}
		return
	}
	for mapLen > 0 {
		var hash [32]byte
		_, err = r.Read(hash[:])
		if err != nil {
			break
		}
		var header RawHeaderData
		_, err = r.Read(header.CurrentHeaderHash[:])
		if err != nil {
			break
		}
		_, err = r.Read(header.Prevhash[:])
		if err != nil {
			break
		}
		_, err = r.Read(header.FileNum[:])
		if err != nil {
			break
		}
		_, err = r.Read(header.Offset[:])
		if err != nil {
			break
		}

		resumeData.nextMap[hash] = header
		mapLen--
	}

	return n, nil
}

func (resumeData *offsetFileResumeData) WriteTo(w io.Writer) (n int64, err error) {
	n = 8
	err = binary.Write(w, binary.BigEndian, resumeData.lastOffsetHeight)
	if err != nil {
		return
	}

	err = binary.Write(w, binary.BigEndian, resumeData.blocksIndexedInLastFile)
	if err != nil {
		return
	}

	err = binary.Write(w, binary.BigEndian, resumeData.fileNum)
	if err != nil {
		return
	}

	err = binary.Write(w, binary.BigEndian, uint32(len(resumeData.nextMap)))
	if err != nil {
		return
	}

	for hash, header := range resumeData.nextMap {
		w.Write(hash[:])
		w.Write(header.CurrentHeaderHash[:])
		w.Write(header.Prevhash[:])
		w.Write(header.FileNum[:])
		w.Write(header.Offset[:])
	}

	return n, nil
}

// OffsetFile acts as an index for block locations since blk*.dat
// files generated by Bitcoin Core has blocks out of order.
type OffsetFile struct {
	offsetFile     *os.File
	resumeDataFile *os.File
	// Map to store Block Header Hashes for sorting purposes
	// blk*.dat files aren't in block order so this is needed
	tip        util.Hash
	dataDir    string
	resumeData *offsetFileResumeData
}

// NewOffsetFile creates a new offset file or restores one.
// Uses the default file locations if empty string are provided.
func NewOffsetFile(dataDir, cOffsetFile, cResumeDataFile string,
	genesis util.Hash) (file *OffsetFile, err error) {
	file = new(OffsetFile)

	if cOffsetFile == "" {
		cOffsetFile = util.OffsetFilePath
	}
	file.offsetFile, err = os.OpenFile(cOffsetFile,
		os.O_CREATE|os.O_RDWR, 0600)
	if err != nil {
		return
	}

	if cResumeDataFile == "" {
		cResumeDataFile = util.OffsetFileResumeDataFilePath
	}
	file.resumeDataFile, err = os.OpenFile(
		cResumeDataFile, os.O_CREATE|os.O_RDWR, 0600)
	if err != nil {
		return
	}
	file.resumeData = &offsetFileResumeData{
		nextMap: make(map[[32]byte]RawHeaderData),
	}

	file.dataDir = dataDir

	// set `file.tip` to the last block or the the genesis hash.
	header, err := file.LastHeader()
	if err != nil {
		// there was no last block (new offsetfile)
		file.tip = genesis
		err = nil
		fmt.Println("new offsetfile created")
	} else {
		// set the tip to the last indexed block
		file.tip = util.Hash{}
		copy(file.tip[:], header.CurrentHeaderHash[:])
		fmt.Printf("offsetfile restored, the tip is %x\n", file.tip)
	}

	return
}

// Close the OffsetFile file descriptors.
func (file *OffsetFile) Close() {
	file.offsetFile.Close()
	file.resumeDataFile.Close()
}

// Build the offset file for the blocks contained in `dataDir`.
// To let `Build` know that there are new blocks, pass true to `newBlocks`.
// Returns a channel that passes the height of the latest block.
func (file OffsetFile) Build(newBlocks, haltRequest, haltAccepted chan bool) chan int32 {
	lastIndexed := make(chan int32)

	go func() {
	build_loop:
		for {
			select {
			case <-newBlocks:
				err := file.build(lastIndexed, haltRequest, haltAccepted)
				if err != nil {
					panic(err)
				}
			case <-haltRequest:
				break build_loop
			}
		}

		close(lastIndexed)
		haltAccepted <- true
	}()

	return lastIndexed
}

// TODO: this is ugly replace it with a golang version
func CopyDir(src, dst string) error {
	cmd := exec.Command("cp", "-r", src, dst)
	return cmd.Run()
}

func checkForBlkFile(blocksDir string, num uint32) (string, bool) {
	fileName := fmt.Sprintf("blk%05d.dat", num)
	fileDir := filepath.Join(blocksDir, fileName)
	_, err := os.Stat(fileDir)
	return fileDir, !os.IsNotExist(err)
}

func (file *OffsetFile) build(lastIndexed chan int32, haltRequest, haltAccepted chan bool) error {
	var haltRequested bool

	indexDir := filepath.Join(file.dataDir, "/index")
	indexCopyDir := filepath.Join(file.dataDir, "/utree/index_copy")

	// copy /index to /index_copy, this is bad bad sadly needed
	// TODO: could we copy to memory?
	err := CopyDir(indexDir, indexCopyDir)
	if err != nil {
		return err
	}

	lvdb, err := OpenIndexFile(indexCopyDir)
	if err != nil {
		return err
	}

	bufDB := BufferDB(lvdb)
	lvdb.Close()
	os.RemoveAll(indexCopyDir)

	// Allocate buffered reader for readRawHeadersFromFile
	// Less overhead to pre allocate and reuse
	bufReader := bufio.NewReaderSize(nil, (1<<20)*128) // 128M
	wr := bufio.NewWriter(nil)

	file.resumeDataFile.Seek(0, 0)
	_, err = file.resumeData.ReadFrom(file.resumeDataFile)
	if err != nil {
		return err
	}
	fmt.Println("Indexing new blocks, resuming at height", file.resumeData.lastOffsetHeight)

offset_loop:
	for fileNum := file.resumeData.fileNum; ; fileNum++ {
		filePath, exists := checkForBlkFile(file.dataDir, fileNum)
		if !exists {
			fmt.Printf("blk file %d doesn't exist; done building offsets until there are new blocks\n",
				fileNum)
			break
		}
		fmt.Printf("Building offsetfile... %d\n", fileNum)

		// grab headers from the .dat file as RawHeaderData type
		rawheaders, err := readRawHeadersFromFile(bufReader, filePath, uint32(fileNum))
		if err != nil {
			return err
		}

		headersToIndex := rawheaders[file.resumeData.blocksIndexedInLastFile:]
		var newOffsetHeight int32
		file.tip, newOffsetHeight, err = writeBlockOffset(
			headersToIndex, file.resumeData.nextMap, wr, file.offsetFile,
			file.resumeData.lastOffsetHeight, file.tip, bufDB)
		if err != nil {
			return err
		}

		file.resumeData.fileNum = fileNum
		// Check if the next blk file exists, and set blocksIndexedInLastFile accordingly
		filePath, exists = checkForBlkFile(file.dataDir, fileNum+1)
		if exists {
			file.resumeData.blocksIndexedInLastFile = 0
			file.resumeData.fileNum = fileNum + 1
		} else {
			file.resumeData.blocksIndexedInLastFile += uint32(newOffsetHeight - file.resumeData.lastOffsetHeight)
		}

		fmt.Printf("done building blk file %d, new tip is %x at height %d. %d blocks where indexed in this iteration.\n",
			fileNum, file.tip, newOffsetHeight, uint32(newOffsetHeight-file.resumeData.lastOffsetHeight))
		file.resumeData.lastOffsetHeight = newOffsetHeight
		if file.resumeData.lastOffsetHeight > 0 {
			select {
			case lastIndexed <- file.resumeData.lastOffsetHeight:
				break
			case haltRequested = <-haltRequest:
				// break from the main offset loop
				break offset_loop
			}
		}
	}

	file.resumeDataFile.Seek(0, 0)
	_, err = file.resumeData.WriteTo(file.resumeDataFile)
	if err != nil {
		return err
	}

	if haltRequested {
		// resume data written, halt accepted.
		fmt.Println("offsetfile: halt accepted")
		haltAccepted <- true
	}

	return nil
}

// LastHeader returns the block header of the last block that was indexed.
func (file *OffsetFile) LastHeader() (header *RawHeaderData, err error) {
	file.offsetFile.Seek(0, 2)
	_, err = file.offsetFile.Seek(-12, 1)
	if err != nil {
		return
	}
	defer file.offsetFile.Seek(0, 0)

	var fileNum uint32
	var offset uint32
	err = binary.Read(file.offsetFile, binary.BigEndian, &fileNum)
	if err != nil {
		return
	}

	err = binary.Read(file.offsetFile, binary.BigEndian, &offset)
	if err != nil {
		return
	}

	blkFile, err := os.Open(filepath.Join(
		file.dataDir, fmt.Sprintf("blk%05d.dat", fileNum)))
	if err != nil {
		return
	}
	defer blkFile.Close()

	header = new(RawHeaderData)
	binary.BigEndian.PutUint32(header.FileNum[:], fileNum)
	binary.BigEndian.PutUint32(header.Offset[:], offset)

	// skip magic bytes and size of block
	_, err = blkFile.Seek(int64(offset+8), 1)
	if err != nil {
		return
	}

	var blockheader [80]byte
	blkFile.Read(blockheader[:])

	copy(header.Prevhash[:], blockheader[4:32+4])

	// create block hash
	// double sha256 needed with Bitcoin
	first := sha256.Sum256(blockheader[:])
	header.CurrentHeaderHash = sha256.Sum256(first[:])

	return
}

/*
Proof file format is somewhat like the blk.dat and rev.dat files.  But it's
always in order!  The offset file is in 8 byte chunks, so to find the proof
data for block 100 (really 101), seek to byte 800 and read 8 bytes.

The proof file is: 4 bytes empty (zeros for now, could do something else later)
4 bytes proof length, then the proof data.

Offset file is: 8 byte int64 offset.  Right now it's all 1 big file, can
change to 4 byte which file and 4 byte offset within file like the blk/rev but
we're not running on fat32 so works OK for now.
*/

// pFileWorker takes in blockproof and height information from the channel
// and writes to disk. MUST NOT have more than one worker as the proofs need to be
// in order
func proofWriterWorker(proofChan chan []byte, newProofChan *sync.Cond,
	fileWait *sync.WaitGroup) {

	// for the pFile
	proofFile, err := os.OpenFile(
		util.PFilePath, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0600)
	if err != nil {
		panic(err)
	}

	offsetFile, err := os.OpenFile(
		util.POffsetFilePath, os.O_APPEND|os.O_CREATE|os.O_RDWR, 0600)
	if err != nil {
		panic(err)
	}

	_, err = offsetFile.Seek(0, 2)
	if err != nil {
		panic(err)
	}

	proofFileLocation, err := proofFile.Seek(0, 2)
	if err != nil {
		panic(err)

	}
	// TODO: optimization - don't write anything to proof file for blocks with
	// no deletions (inputs).  Lots of em in testnet.  Not so many on mainnet
	// I guess.  But in testnet would save millions *8 bytes.
	for {
		pbytes := <-proofChan
		// write to offset file first
		err = binary.Write(offsetFile, binary.BigEndian, proofFileLocation)
		if err != nil {
			fmt.Printf(err.Error())
			return
		}

		// write to proof file
		// first write big endian proof size int64
		err = binary.Write(proofFile, binary.BigEndian, int64(len(pbytes)))
		if err != nil {
			fmt.Printf(err.Error())
			return
		}
		proofFileLocation += 8

		// then write the proof
		written, err := proofFile.Write(pbytes)
		if err != nil {
			fmt.Printf(err.Error())
			return
		}
		proofFileLocation += int64(written)

		fileWait.Done()

		// tell the pushBlocks routines of the blockServer that there are new blocks to serve
		newProofChan.Broadcast()
	}
}

// readRawHeadersFromFile reads only the headers from the given .dat file
func readRawHeadersFromFile(bufReader *bufio.Reader, fileDir string,
	fileNum uint32) ([]RawHeaderData, error) {
	var blockHeaders []RawHeaderData

	f, err := os.Open(fileDir)
	if err != nil {
		panic(err)
	}
	defer f.Close()

	fStat, err := f.Stat()
	if err != nil {
		panic(err)
	}
	fSize := fStat.Size()

	bufReader.Reset(f)

	var buf [88]byte    // buffer for magicbytes, size, and 80 byte header
	offset := uint32(0) // where the block is located from the beginning of the file

	// until offset is at the end of the file
	for int64(offset) != fSize {
		b := new(RawHeaderData)
		binary.BigEndian.PutUint32(b.FileNum[:], fileNum)
		binary.BigEndian.PutUint32(b.Offset[:], offset)

		_, err := bufReader.Read(buf[:])
		if err != nil {
			panic(err)
		}
		// check if Bitcoin magic bytes were read
		if util.CheckMagicByte(buf[:4]) == false {
			break
		}

		// read the 4 byte size of the load of the block
		size := binary.LittleEndian.Uint32(buf[4:8])

		// add 8bytes for the magic bytes (4bytes) and size (4bytes)
		offset = offset + size + uint32(8)

		copy(b.Prevhash[:], buf[12:12+32])

		// create block hash
		// double sha256 needed with Bitcoin
		first := sha256.Sum256(buf[8 : 8+80])
		b.CurrentHeaderHash = sha256.Sum256(first[:])

		// offset for the next block from the current position
		bufReader.Discard(int(size) - 80)

		blockHeaders = append(blockHeaders, *b)
	}

	return blockHeaders, nil
}

// Sorts and writes the block offset from the passed in blockHeaders.
func writeBlockOffset(
	blockHeaders []RawHeaderData, //        All headers from the select .dat file
	nextMap map[[32]byte]RawHeaderData, //  Map to save the current block hash
	wr *bufio.Writer, //buffered writer
	offsetFile *os.File, //                 File to save the sorted blocks and locations to
	tipnum int32, //                          Current block it's on
	tip util.Hash, //                Current hash of the block it's on
	bufMap map[[32]byte]uint32) (
	util.Hash, int32, error) {

	wr.Reset(offsetFile)
	offsetFile.Seek(int64(12*tipnum), 0)

	for _, b := range blockHeaders {
		if len(nextMap) > 10000 { //Just a random big number
			fmt.Println("Dead end tip. Exiting...")
			break
		}

		// The block's Prevhash doesn't match the
		// previous block header. Add to map.
		// Searches until it finds a hash that does.
		if b.Prevhash != tip {
			nextMap[b.Prevhash] = b
			continue
		}

		var ok bool
		b.UndoPos, ok = bufMap[b.CurrentHeaderHash]
		if !ok {
			fmt.Printf("block in blk file with header: %x\nexists without"+
				" a corresponding rev block. done reading headers.\n", b.CurrentHeaderHash)
			continue
		}
		undoOffset := make([]byte, 4)
		binary.BigEndian.PutUint32(undoOffset, b.UndoPos)

		// Write the .dat file name and the
		// offset the block can be found at
		wr.Write(b.FileNum[:])
		wr.Write(b.Offset[:])
		// write undoblock offset
		wr.Write(undoOffset)

		// set the tip to current block's hash
		tip = b.CurrentHeaderHash
		tipnum++

		// check for next blocks in map
		// same thing but with the stored blocks
		// that we skipped over
		stashedBlock, ok := nextMap[tip]
		for ok {
			stashedBlock.UndoPos, ok = bufMap[stashedBlock.CurrentHeaderHash]
			if !ok {
				fmt.Printf("block in blk file with header: %x\nexists without"+
					" a corresponding rev block. done reading headers.\n", stashedBlock.CurrentHeaderHash)
				break
			}
			sUndoOffset := make([]byte, 4)
			binary.BigEndian.PutUint32(sUndoOffset, stashedBlock.UndoPos)

			// Write the .dat file name and the
			// offset the block can be found at
			wr.Write(stashedBlock.FileNum[:])
			wr.Write(stashedBlock.Offset[:])
			// write undoblock offset
			wr.Write(sUndoOffset)

			// set the tip to current block's hash
			tip = stashedBlock.CurrentHeaderHash
			tipnum++

			// remove the written current block
			delete(nextMap, stashedBlock.Prevhash)

			// move to the next block
			stashedBlock, ok = nextMap[tip]
		}
	}

	wr.Flush()
	return tip, tipnum, nil
}
